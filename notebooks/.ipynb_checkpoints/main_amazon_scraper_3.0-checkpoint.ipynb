{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already satisfied: selectorlib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.16.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.23.0)\n",
      "Requirement already satisfied: click>=5.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from flask) (7.1.2)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from flask) (2.11.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from flask) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from flask) (1.0.1)\n",
      "Requirement already satisfied: parsel>=1.5.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from selectorlib) (1.6.0)\n",
      "Requirement already satisfied: pyyaml>=3.12 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from selectorlib) (5.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from Jinja2>=2.10.1->flask) (1.1.1)\n",
      "Requirement already satisfied: six>=1.6.0 in /Users/ChristieFung/Library/Python/3.8/lib/python/site-packages (from parsel>=1.5.1->selectorlib) (1.14.0)\n",
      "Requirement already satisfied: w3lib>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from parsel>=1.5.1->selectorlib) (1.22.0)\n",
      "Requirement already satisfied: cssselect>=0.9 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from parsel>=1.5.1->selectorlib) (1.1.0)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from parsel>=1.5.1->selectorlib) (4.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask selectorlib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "header={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36', 'referrer': 'https://google.com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.html import fromstring\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'45.235.163.35:33265', '81.95.230.211:3128', '103.243.82.198:37358', '103.221.254.125:51630', '110.74.221.18:53348', '107.190.148.202:50854', '118.174.220.32:51399'}\n"
     ]
    }
   ],
   "source": [
    "proxies = get_proxies()\n",
    "print(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-feb8ef598a82>:61: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(1, npages+1)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f866bd3641414184b0dae8e1cf0636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/s?k=normal+face&page=1\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=2\n",
      "Request #10\n",
      "Failed to parse in this page: 2\n",
      "https://www.amazon.com/s?k=normal+face&page=3\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=4\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=5\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=6\n",
      "Request #10\n",
      "Failed to parse in this page: 6\n",
      "https://www.amazon.com/s?k=normal+face&page=7\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=8\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=9\n",
      "Request #10\n",
      "Failed to parse in this page: 9\n",
      "https://www.amazon.com/s?k=normal+face&page=10\n",
      "Request #10\n",
      "Failed to parse in this page: 10\n",
      "https://www.amazon.com/s?k=normal+face&page=11\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=12\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=13\n",
      "Request #10\n",
      "Failed to parse in this page: 13\n",
      "https://www.amazon.com/s?k=normal+face&page=14\n",
      "Request #10\n",
      "Failed to parse in this page: 14\n",
      "https://www.amazon.com/s?k=normal+face&page=15\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=16\n",
      "Request #10\n",
      "Failed to parse in this page: 16\n",
      "https://www.amazon.com/s?k=normal+face&page=17\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=18\n",
      "Request #10\n",
      "Failed to parse in this page: 18\n",
      "https://www.amazon.com/s?k=normal+face&page=19\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=normal+face&page=20\n",
      "Request #10\n",
      "\n",
      "Total number of products identified for 20 pages:119\n"
     ]
    }
   ],
   "source": [
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "\n",
    "#If you are copy pasting proxy ips, put in the list below\n",
    "#proxies = ['88.99.10.254:1080', '200.52.141.162:53281', '61.9.82.34:37892', '181.118.167.104:80', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "proxies = get_proxies()\n",
    "proxy_pool = cycle(proxies)\n",
    "\n",
    "def get_product_asins(url_base, search_query):\n",
    "    \"\"\"\n",
    "    ## Function to scrape product name and asin no (as the file path is similar)\n",
    "    \"\"\"\n",
    "    url= \"%s%s\" %(url_base, search_query)\n",
    "    print(url)\n",
    "    \n",
    "    for i in range(1,11):\n",
    "    #Get a proxy from the pool\n",
    "        proxy = next(proxy_pool)\n",
    "    print(\"Request #%d\"%i)\n",
    "\n",
    "    # Get the data\n",
    "    page=requests.get(url, headers=header,proxies={\"http\": proxy, \"https\": proxy})  \n",
    "    if page.status_code==200:\n",
    "        return page                                #returns the page if there is no error\n",
    "    else:\n",
    "        return \"Error\"\n",
    "    \n",
    "sites = {\n",
    "    'amazon.com':{\n",
    "        'url_base': \"https://www.amazon.com/s?k=\",\n",
    "        'asin_div': {'data-asin':True},\n",
    "        'product_name_span': {'class':'a-size-base-plus a-color-base a-text-normal'}\n",
    "    },\n",
    "    'amazon.in':{\n",
    "        'url_base': \"https://www.amazon.in/s?k=\",\n",
    "        #'asin_div': {'class':['sg-col-4-of-24 sg-col-4-of-12 sg-col-4-of-36 s-result-item s-asin sg-col-4-of-28 sg-col-4-of-16 AdHolder sg-col sg-col-4-of-20 sg-col-4-of-32']},\n",
    "        'asin_div': {'data-asin':True},\n",
    "        'product_name_span': {'class':'a-size-base-plus a-color-base a-text-normal'}\n",
    "    },\n",
    "}    \n",
    "\n",
    "\n",
    "# -------------------- custom parameters\n",
    "site = sites['amazon.com']\n",
    "npages = 20\n",
    "query = 'normal+face'\n",
    "\n",
    "\n",
    "# ------------------------- start\n",
    "products = OrderedDict()\n",
    "for i in tqdm(range(1, npages+1)):\n",
    "    try:\n",
    "    \n",
    "        # Get the respone and create bf4\n",
    "        response=get_product_asins(site['url_base'],'%s&page=%s' %(query,str(i)))     #iterates through multiple pages of the search products\n",
    "        soup=BeautifulSoup(response.content)\n",
    "\n",
    "        # parse the asins (each product will have a unique asin)\n",
    "        for p in soup.findAll('div', attrs=site['asin_div']):\n",
    "            asin = p['data-asin']\n",
    "\n",
    "            # parse the product name. If there is no product name\n",
    "            pn = p.find('span', attrs=site['product_name_span'])\n",
    "            if pn:\n",
    "                pn = pn.text\n",
    "                products[asin] = pn\n",
    "    except:\n",
    "        print('Failed to parse in this page:', i)\n",
    "            \n",
    "# print and check\n",
    "print('Total number of products identified for %s pages:%s' %(npages, len(products)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "fn = '%s_product_asins' %dt\n",
    "with open('../data/%s.pkl' %fn, 'wb') as f:\n",
    "    pickle.dump({'asin':products}, f)\n",
    "\n",
    "    #pkl = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B07WSS5M4Z', 'B07DW1HCZ4', 'B07XFBBY53', 'B073T4P56P', 'B07SZ6MCB7', 'B074ZHMMD4', 'B07TJ7P39Y', 'B07KJFCS8P', 'B07V7G7MBK', 'B00R9KZQPI', 'B087NMTT81', 'B01IR9H86Y', 'B077TQR6ZW', 'B00FSBUQMI', 'B07TJDR8B3', 'B00F97FHAW', 'B008EPPIQ4', 'B01F7SUFFE', 'B07R55ZLXB', 'B07PZWVJM6', 'B082YJL1PW', 'B000TV9RW2', 'B07VXSGXYG', 'B086445QKW', 'B07GYR25M4', 'B017U0D1BC', 'B078PRSTLW', 'B07V1MPG8N', 'B00VMYKCL0', 'B07VWSN95S', 'B00NR1YQK4', 'B00EQWWYQ6', 'B004D2826K', 'B0822VY9BB', 'B07P7Y5BGF', 'B07PHCCDNK', 'B003RF82UK', 'B00APQDPAQ', 'B079NW2RM2', 'B07D5GQ7RD', 'B07WJ95RZ3', 'B00SNPCSUY', 'B07K1MH35J', 'B002P3L99G', 'B07J4X9C51', '1623159342', 'B01685610I', 'B000U5O650', 'B001IM5VT4', 'B074PLS8BK', 'B01M4MCUAF', 'B001ET79H8', 'B01N34XW93', 'B01K7MW152', 'B08246V6CR', 'B07C254PLQ', 'B00GCRN8FE', 'B0767YD5GS', 'B00UREAGU8', 'B01CTBQAP6', 'B07YHCFCBZ', 'B001MV5UQ0', 'B07CRY1G6C', 'B00DYYKSRO', 'B01EGRLH40', 'B07D236GQY', 'B01MG4PSK4', 'B003UPIUJK', 'B07VXH6FWW', 'B07D5NND22', 'B07WKDR1BL', 'B07PZ8KYZY', 'B003JSEGNC', 'B07N61HRBS', 'B084CVT521', 'B07ZX7SPH1', 'B0888TWQJ8', 'B00809ERAM', 'B07WKM8FH2', 'B079X55W28', 'B0006O2LV6', 'B00HDQJPC8', 'B07YBJ9Q17', 'B074QQDDT7', 'B00SETTZGE', 'B01N5D7A4D', 'B01HOHBS7K', 'B071DTFVLW', 'B07D27CRDD', 'B07TB2SQXM', 'B010TV2SL8', 'B075WZQ8RF', 'B06XQXS7XC', 'B07FL583TR', 'B07PDF4ZN5', 'B01M1HIIUT', 'B082LP8DMY', 'B07D96ZBBW', 'B002LC9OES', 'B00BUS0RHW', 'B089NBR8X4', 'B06WW147HL', 'B01D1DQSIA', 'B00ZPVQLZC', 'B01MQFH8AX', 'B01H3YTY8Y', 'B01N4X8HPF', 'B06WRPJW3V', 'B081FVRG34', 'B07B57TPCX', 'B00SXDS6JI', 'B00Q5ERAWQ', 'B00U1YCRD8', 'B07PY3LCPZ', 'B07P8YL4P7', 'B07VFC9VFY', 'B071LHHNL1', 'B07VQXQHJ1', 'B07JKF4G32', 'B01BPACNX0', 'B088FGLJHM', 'B07CNPHQ2S', 'B000NWAOHE', 'B00N738GU8', 'B083315GCN', 'B015D6BA0Y', 'B006YQ7Z3E', 'B002HF07MC', 'B000EPA4GQ', 'B00BNAPD30', 'B07K3261ZD', 'B07BSV1T35', 'B0046VJV36', 'B073RRCXCT', 'B07MQG9D1Z', 'B003FBOJLW', 'B07SQXPW35', 'B079XYF42F', 'B083LYFCQ6', 'B07M9D2Y74', 'B07C5G28ZL', 'B017PCGAXQ', 'B07NDXJL3V', 'B017OY5ZZE', 'B08533MJMQ', 'B07DSRVPKJ', 'B073T5H2X5', 'B0741CQTFL', 'B082YHN78H', 'B0765985BH', 'B07659X297', 'B07CK9L2J5', 'B077NK1KDD', 'B07Y46CPL4', 'B07DK3KZFN', 'B07CT11SWF', 'B077399ZHS', 'B07ZJVL8KV', 'B001UOQ30Q', 'B0197XR9QS', 'B0776X8G8D', 'B06XS25J8C']\n"
     ]
    }
   ],
   "source": [
    "# check the data/ folder and identify the saved file   #03-06-2020_23-45-41_product_asins oily #04-06-2020_01-01-51_product_asins.pkl dry\n",
    "fn = '08-06-2020_14-44-46_product_asins'     #08-06-2020_14-44-46_product_asins.pkl oily from us amazon; 08-06-2020_14-21-28_product_asins dry US\n",
    "with open('../data/%s.pkl' %fn, 'rb') as f: #08-06-2020_15-06-53_product_asins.pkl normal US\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "# Get all the ASINS\n",
    "asins = list(data['asin'].keys())\n",
    "print(asins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    API_KEY = '0bc9fbde7ebc64acb5b24494bb658655'\n",
    "    URL_TO_SCRAPE = 'url'\n",
    "payload = {'api_key': API_KEY, 'url': URL_TO_SCRAPE}\n",
    "r = requests.get('http://api.scraperapi.com', params=payload, timeout=60)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request #1\n",
      "Request #2\n",
      "Request #3\n",
      "Request #4\n",
      "Request #5\n",
      "Request #6\n",
      "Request #7\n",
      "Request #8\n",
      "Request #9\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=B07WSS5M4Z\n",
      "Failed to get the url for this product: B07WSS5M4Z\n",
      "Request #1\n",
      "Request #2\n",
      "Request #3\n",
      "Request #4\n",
      "Request #5\n",
      "Request #6\n",
      "Request #7\n",
      "Request #8\n",
      "Request #9\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=B07DW1HCZ4\n",
      "Failed to get the url for this product: B07DW1HCZ4\n",
      "Request #1\n",
      "Request #2\n",
      "Request #3\n",
      "Request #4\n",
      "Request #5\n",
      "Request #6\n",
      "Request #7\n",
      "Request #8\n",
      "Request #9\n",
      "Request #10\n",
      "https://www.amazon.com/s?k=B07XFBBY53\n",
      "Failed to get the url for this product: B07XFBBY53\n",
      "Number of review links: 0\n"
     ]
    }
   ],
   "source": [
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "#If you are copy pasting proxy ips, put in the list below\n",
    "#proxies = ['88.99.10.254:1080', '200.52.141.162:53281', '61.9.82.34:37892', '181.118.167.104:80', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "proxies = get_proxies()\n",
    "proxy_pool = cycle(proxies)\n",
    "\n",
    "\n",
    "def get_review_url(base_url, query):\n",
    "    \"\"\"\n",
    "    Function to scrape link of the All customer reviews to acess all the reviews\n",
    "    \"\"\"\n",
    "     \n",
    "    for i in range(1,11):\n",
    "    #Get a proxy from the pool\n",
    "        proxy = next(proxy_pool)\n",
    "        print(\"Request #%d\"%i)\n",
    "    \n",
    "    # query\n",
    "#    if __name__ == '__main__':\n",
    "#        API_KEY = '0bc9fbde7ebc64acb5b24494bb658655'\n",
    "#        URL_TO_SCRAPE = 'url'\n",
    "#        payload = {'api_key': API_KEY, 'url': URL_TO_SCRAPE}\n",
    "#        page = requests.get('http://api.scraperapi.com', params=payload, timeout=60)\n",
    "\n",
    "    url= \"%s%s\" %(base_url, query)\n",
    "    print(url)\n",
    "    page=requests.get(url, headers=header,proxies={\"http\": proxy, \"https\": proxy})  \n",
    "    if page.status_code==200:\n",
    "        return page                                #returns the page if there is no error\n",
    "    else:\n",
    "        return \"Error\"\n",
    "    \n",
    "    # check status\n",
    "    if page.status_code==200:\n",
    "        return page                           \n",
    "    else:\n",
    "        return \"Error\"\n",
    "\n",
    "sites = {\n",
    "    'amazon.com':{\n",
    "        'url_base': \"https://www.amazon.com/s?k=\",\n",
    "        'asin_div': {'data-asin':True},\n",
    "        'product_name_span': {'class':'a-size-base-plus a-color-base a-text-normal'}\n",
    "    },\n",
    "    'amazon.in':{\n",
    "        'url_base': \"https://www.amazon.in/s?k=\",\n",
    "        #'asin_div': {'class':['sg-col-4-of-24 sg-col-4-of-12 sg-col-4-of-36 s-result-item s-asin sg-col-4-of-28 sg-col-4-of-16 AdHolder sg-col sg-col-4-of-20 sg-col-4-of-32']},\n",
    "        'asin_div': {'data-asin':True},\n",
    "        'product_name_span': {'class':'a-size-base-plus a-color-base a-text-normal'}\n",
    "    },\n",
    "}    \n",
    "\n",
    "\n",
    "# ------------------------- custom parameters\n",
    "site = sites['amazon.com']\n",
    "nproducts = 3 # THis can adjust based on how many products I want to check\n",
    "                    # ***this cell take a lot of time to run\n",
    "                    # started small, e.g., nproducts = 5 to see if everything work,\n",
    "                    # then to all the available ASINS len(asins)\n",
    "\n",
    "\n",
    "# ------------------------- start\n",
    "review_links = {}\n",
    "for i in range(0, nproducts):\n",
    "    try:\n",
    "        r=get_review_url(site['url_base'], asins[i])\n",
    "\n",
    "        # bf4\n",
    "        soup=BeautifulSoup(r.content)\n",
    "        tag = soup.find('a',attrs=site['tag_a'])\n",
    "        if tag:\n",
    "            review_links[asins[i]] = tag['href']\n",
    "    except:\n",
    "        print('Failed to get the url for this product:', asins[i])\n",
    "\n",
    "# check\n",
    "print('Number of review links:', len(review_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
