{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon scraper\n",
    "- Reference: https://github.com/vijeshs/Web-Scraping-/blob/master/Web%20Scraping-JBL%20speaker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fake_useragent\n",
      "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
      "Building wheels for collected packages: fake-useragent\n",
      "  Building wheel for fake-useragent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13487 sha256=e270201b5a34ac2a1d12165f1a68cf1817597aeb9d30cdd4240c689473fa1405\n",
      "  Stored in directory: /Users/ChristieFung/Library/Caches/pip/wheels/a0/b8/b7/8c942b2c5be5158b874a88195116b05ad124bac795f6665e65\n",
      "Successfully built fake-useragent\n",
      "Installing collected packages: fake-useragent\n",
      "Successfully installed fake-useragent-0.1.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "header={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This saves all the product name and the asin no\n",
    "- The Amazon Standard Identification Number (asin no) is the number used for specific product search\n",
    "- Given a search query, loop through multiple pages of amazon and get the coressponding products\n",
    "- Using amazon.in, not amazon.com because amazon.com parsing may fail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-2dc955683d3e>:38: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(1, npages+1)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1281368762a24eaab75b64628c8afd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.in/s?k=normal+skin+lotion&page=1\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=2\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=3\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=4\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=5\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=6\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=7\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=8\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=9\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=10\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=11\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=12\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=13\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=14\n",
      "https://www.amazon.in/s?k=normal+skin+lotion&page=15\n",
      "\n",
      "Total number of products identified for 15 pages:0\n"
     ]
    }
   ],
   "source": [
    "def get_product_asins(url_base, search_query):\n",
    "    \"\"\"\n",
    "    ## Function to scrape product name and asin no (as the file path is similar)\n",
    "    \"\"\"\n",
    "    url=\"%s%s\" %(url_base, search_query)\n",
    "    print(url)\n",
    "    \n",
    "    # Get the data\n",
    "    page=requests.get(url,headers=header)  \n",
    "    if page.status_code==200:\n",
    "        return page                                #returns the page if there is no error\n",
    "    else:\n",
    "        return \"Error\"\n",
    "    \n",
    "sites = {\n",
    "    'amazon.com':{\n",
    "        'url_base': \"https://www.amazon.com/s?k=\",\n",
    "        'asin_div': {'data-asin':True},\n",
    "        'product_name_span': {'class':'a-size-base-plus a-color-base a-text-normal'}\n",
    "    },\n",
    "    'amazon.in':{\n",
    "        'url_base': \"https://www.amazon.in/s?k=\",\n",
    "        #'asin_div': {'class':['sg-col-4-of-24 sg-col-4-of-12 sg-col-4-of-36 s-result-item s-asin sg-col-4-of-28 sg-col-4-of-16 AdHolder sg-col sg-col-4-of-20 sg-col-4-of-32']},\n",
    "        'asin_div': {'data-asin':True},\n",
    "        'product_name_span': {'class':'a-size-base-plus a-color-base a-text-normal'}\n",
    "    },\n",
    "}    \n",
    "\n",
    "\n",
    "# -------------------- custom parameters\n",
    "site = sites['amazon.in']\n",
    "npages = 15\n",
    "query = 'normal+skin+lotion'\n",
    "\n",
    "\n",
    "# ------------------------- start\n",
    "products = OrderedDict()\n",
    "for i in tqdm(range(1, npages+1)):\n",
    "    try:\n",
    "    \n",
    "        # Get the respone and create bf4\n",
    "        response=get_product_asins(site['url_base'],'%s&page=%s' %(query,str(i)))     #iterates through multiple pages of the search products\n",
    "        soup=BeautifulSoup(response.content)\n",
    "\n",
    "        # parse the asins (each product will have a unique asin)\n",
    "        for p in soup.findAll('div', attrs=site['asin_div']):\n",
    "            asin = p['data-asin']\n",
    "\n",
    "            # parse the product name. If there is no product name\n",
    "            pn = p.find('span', attrs=site['product_name_span'])\n",
    "            if pn:\n",
    "                pn = pn.text\n",
    "                products[asin] = pn\n",
    "    except:\n",
    "        print('Failed to parse in this page:', i)\n",
    "            \n",
    "# print and check\n",
    "print('Total number of products identified for %s pages:%s' %(npages, len(products)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save\n",
    "- save the information in case something break and I lost all the webscraping information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "fn = '%s_product_asins' %dt\n",
    "with open('../data/%s.pkl' %fn, 'wb') as f:\n",
    "    pickle.dump({'asin':products}, f)\n",
    "\n",
    "    #pkl = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hack the asin_no\n",
    "# Asin_no=['B00EH99VY6',\n",
    "#  'B07STDDDGF',\n",
    "#  'B07LCQW2RC',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify each product (ASIN) review link\n",
    "- One example of review link is (The overaall review link): \n",
    "https://www.amazon.in//Lacto-Calamine-Daily-Lotion-Balance/product-reviews/B00EH99VY6/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews&pageNumber=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved ASINs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data/ folder and identify the saved file   #03-06-2020_23-45-41_product_asins oily #04-06-2020_01-01-51_product_asins.pkl dry\n",
    "fn = '04-06-2020_02-06-03_product_asins' \n",
    "with open('../data/%s.pkl' %fn, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "# Get all the ASINS\n",
    "asins = list(data['asin'].keys())\n",
    "print(asins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the review link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_url(base_url, query):\n",
    "    \"\"\"\n",
    "    Function to scrape link of the All customer reviews to acess all the reviews\n",
    "    \"\"\"\n",
    "    # query\n",
    "    url=\"%s%s\" %(base_url, query)\n",
    "    print(url)\n",
    "    page=requests.get(url, headers=header)\n",
    "    \n",
    "    # check status\n",
    "    if page.status_code==200:\n",
    "        return page                           \n",
    "    else:\n",
    "        return \"Error\"\n",
    "\n",
    "sites = {\n",
    "    'amazon.in':{\n",
    "        'url_base': \"https://www.amazon.in/dp/\",\n",
    "        'tag_a': {'data-hook':'see-all-reviews-link-foot'}\n",
    "    },\n",
    "}  \n",
    "\n",
    "# ------------------------- custom parameters\n",
    "site = sites['amazon.in']\n",
    "nproducts = len(asins) # THis can adjust based on how many products you want to check\n",
    "                    # FYI: this cell take a lot of time to run, you may want to\n",
    "                    # start small, e.g., nproducts = 5, and see if everything work,\n",
    "                    # then go to all the available ASINS\n",
    "                    # len(asins)\n",
    "\n",
    "\n",
    "# ------------------------- start\n",
    "review_links = {}\n",
    "for i in range(0, nproducts):\n",
    "    try:\n",
    "        r=get_review_url(site['url_base'], asins[i])\n",
    "\n",
    "        # bf4\n",
    "        soup=BeautifulSoup(r.content)\n",
    "        tag = soup.find('a',attrs=site['tag_a'])\n",
    "        if tag:\n",
    "            review_links[asins[i]] = tag['href']\n",
    "    except:\n",
    "        print('Failed to get the url for this product:', asins[i])\n",
    "\n",
    "# check\n",
    "print('Number of review links:', len(review_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save\n",
    "save the information in case something break and I lost all the webscraping information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "fn = '%s_product_review_link' %dt\n",
    "with open('../data/%s.pkl' %fn, 'wb') as f:\n",
    "    pickle.dump({'review_link':review_links}, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each product, scrape the review, rating, and images\n",
    "- install tqdm\n",
    "- install https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "- Restart your notebook after installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the review link of every product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '04-06-2020_01-18-20_product_review_link'     #04-06-2020_00-03-58_product_review_link oily\n",
    "with open('../data/%s.pkl' %fn, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "# Get all the ASINS\n",
    "links = data['review_link']\n",
    "links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hack the link\n",
    "# # Find a particlar review page link and put it here\n",
    "# links=['/Lacto-Calamine-Daily-Lotion-Balance/product-reviews/B00EH99VY6/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&amp;reviewerType=all_reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the reivew and rating for each products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_review_img(url_base, query):\n",
    "    url=\"%s%s\" %(url_base, query)\n",
    "    print(url)\n",
    "    page=requests.get(url,headers=header)\n",
    "    if page.status_code==200:\n",
    "        return page\n",
    "    else:\n",
    "        return \"Error\" \n",
    "\n",
    "sites = {\n",
    "    'amazon.in':{\n",
    "        'url_base': \"https://www.amazon.in/\",\n",
    "    },\n",
    "}\n",
    "    \n",
    "    \n",
    "# ------------------------- custom parameters\n",
    "npages = 10 # number of page of reviews to visit\n",
    "site = sites['amazon.in']\n",
    "\n",
    "\n",
    "# ------------------------- Start\n",
    "# lists to store different information\n",
    "asins = []\n",
    "products = [] # product names\n",
    "reviews = [] # reviews of the products\n",
    "ratings = [] # rating of a product of a review\n",
    "img_links = [] # all the image links related to a review\n",
    "\n",
    "# for k in range(len(links)): \n",
    "for k, asin in enumerate(links.keys()):\n",
    "    for i in tqdm(range(1, npages)):   \n",
    "        try:\n",
    "            # Construct the bf4\n",
    "            response=get_review_img(site['url_base'], links[asin]+'&pageNumber='+str(i))\n",
    "            soup=BeautifulSoup(response.content)\n",
    "\n",
    "            # Get the product name\n",
    "            pn = soup.find(\"a\", attrs={'data-hook':'product-link'}).text\n",
    "\n",
    "            # Get each review for this particular product\n",
    "            for review in soup.findAll(\"div\", attrs={'data-hook':'review'}):\n",
    "                # identify if a review contains any image. If so, then we will\n",
    "                # save the image(s) and assign ids to images\n",
    "                imgs = review.findAll(\"img\", attrs={'class':'review-image-tile'})\n",
    "                if len(imgs) > 0:\n",
    "                    # get the review text\n",
    "                    text = review.find(\"span\", attrs={'data-hook':'review-body'}).text.replace('\\n',\"\")\n",
    "\n",
    "                    # get the rating\n",
    "                    rating = review.find(\"i\", attrs={'data-hook':\"review-star-rating\"}).text\n",
    "\n",
    "                    # get the image link (I don't save the images for now since it will take time).\n",
    "                    # As long as we get the image links, we can write another function to read the csv\n",
    "                    # and save the images somewhere\n",
    "                    ilinks = [img['src'].replace(\"._SY88\", \"\") for img in imgs]\n",
    "                    ilinks = \",\".join(ilinks)\n",
    "\n",
    "                    # append\n",
    "                    asins.append(asin)\n",
    "                    products.append(pn)\n",
    "                    reviews.append(text)\n",
    "                    ratings.append(rating)\n",
    "                    img_links.append(ilinks)\n",
    "        except:\n",
    "            print(\"Failed to get the review and images for this ASIN (%s) in this page (%s)\" %(asin, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the information into pandas and then into csv\n",
    "- you should see a new file created in the data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructe the data frame\n",
    "df = pd.DataFrame({'pid': list(range(0, len(asins))),\n",
    "                    'ASIN': asins,\n",
    "                    'product_name':products,\n",
    "                   'review': reviews,\n",
    "                   'rating': ratings,\n",
    "                   'img_link': img_links\n",
    "                  })\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "df.to_csv('../data/%s_amazon_review_with_image.csv' %dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
